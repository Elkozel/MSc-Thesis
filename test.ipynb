{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6402e6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"dask[complete]\"\n",
    "!pip install networkx\n",
    "!pip install matplotlib\n",
    "!pip install torch_geometric\n",
    "!pip install pyg_lib torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.6.0+cpu.html\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "!pip install pymongo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84571de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "# Load LANL\n",
    "with ProgressBar():\n",
    "    print(\"Loading auth logs\")\n",
    "    authLogs = dd.read_csv(\n",
    "        \"/data/LANL/auth.txt\",\n",
    "        names=[\n",
    "            \"time\",\n",
    "            \"source user\",\n",
    "            \"destination user\",\n",
    "            \"source computer\",\n",
    "            \"destination computer\",\n",
    "            \"authentication type\",\n",
    "            \"logon type\",\n",
    "            \"authentication orientation\",\n",
    "            \"success/failure\"\n",
    "        ],\n",
    "        header=None\n",
    "    )\n",
    "\n",
    "    print(\"Loading redteam logs\")\n",
    "    redLogs = dd.read_csv(\"/data/LANL/redteam.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd4f12eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_auth(row):\n",
    "    row[\"_index\"] = \"index\"\n",
    "    row[\"timestamp\"] = \"hello\"\n",
    "    row[\"file\"] = \"auth\"\n",
    "    return row\n",
    "\n",
    "s = authLogs.apply(apply_auth, axis=1, meta={'time': 'int64', 'source user': 'object', 'destination user': 'object', 'source computer': 'object', 'destination computer': 'object', 'authentication type': 'object', 'logon type': 'object', 'authentication orientation': 'object', 'success/failure': 'object', '_index': 'object', 'timestamp': 'object', 'file': 'object'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccba57c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = {\n",
    "    \"Column1\": [1, 2],\n",
    "    \"Column2\": [\"A\", \"B\"],\n",
    "    \"Column3\": [True, False]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "addition = {\n",
    "    \"_index\": \"Hello\",\n",
    "    \"_source\": \"HJello\"\n",
    "}\n",
    "for row in df.to_dict(orient=\"records\"):\n",
    "    for key, value in addition.items():\n",
    "        row[key] = value\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268e5058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch, helpers\n",
    "\n",
    "# Connect to Elasticsearch\n",
    "es = Elasticsearch(\"https://localhost:9200\", \n",
    "                   api_key=\"MGZGTDhaVUJHWEpfZm5CYVB1bXo6dXBxVk5ucF9Rc3F6dWh5RjVRVDQzUQ==\", \n",
    "                   verify_certs=False, \n",
    "                   ssl_show_warn=False,\n",
    "                   headers={\"Content-Type\": \"application/json\", \"Accept\": \"application/json\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96a7d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ElasticDataFetcher():\n",
    "    \"\"\"\n",
    "    A generator-like class that retrieves all records from an Elasticsearch index\n",
    "    and supports the len() function to get the total number of matching records.\n",
    "\n",
    "    Args:\n",
    "        es (Elasticsearch): The Elasticsearch client instance.\n",
    "        index_name (str): The name of the Elasticsearch index.\n",
    "        query (dict): The query to filter records.\n",
    "        pagination (int, optional): The number of records to fetch per request. Defaults to 10000.\n",
    "    \"\"\"\n",
    "    def __init__(self, es, index_name, query, pagination=10000):\n",
    "        self.es = es\n",
    "        self.index_name = index_name\n",
    "        self.query = query\n",
    "        self.pagination = pagination\n",
    "        self.matchcount = None # The count of matching records\n",
    "        self._total_count = None  # Cache the total count of matching records\n",
    "\n",
    "    def __iter__(self):\n",
    "        search_after = None  # Used for pagination to fetch the next set of records.\n",
    "\n",
    "        while True:\n",
    "            # Perform a search request with pagination and sorting.\n",
    "            resp = self.es.search(\n",
    "                index=self.index_name,\n",
    "                query=self.query,\n",
    "                size=self.pagination,\n",
    "                search_after=search_after,\n",
    "                sort=[\n",
    "                    {\"timestamp\": \"asc\"},  # Sort by datetime in ascending order.\n",
    "                ],\n",
    "            )\n",
    "\n",
    "            # If no records are returned, exit the loop.\n",
    "            if len(resp.body[\"hits\"][\"hits\"]) == 0:\n",
    "                return\n",
    "\n",
    "            # Update the search_after value for the next request.\n",
    "            search_after = resp.body[\"hits\"][\"hits\"][-1][\"sort\"]\n",
    "\n",
    "            # Yield each record's source data.\n",
    "            for record in resp.body[\"hits\"][\"hits\"]:\n",
    "                yield record[\"_source\"]\n",
    "    \n",
    "    def __len__(self):\n",
    "        # Fetch the length if not previously fetched\n",
    "        if self.matchcount == None:\n",
    "            resp = self.es.count(\n",
    "                index=self.index_name,\n",
    "                query=self.query\n",
    "            )\n",
    "            self.matchcount = resp.body[\"count\"]\n",
    "        \n",
    "        return self.matchcount\n",
    "    \n",
    "query = {\n",
    "    \"range\": {\n",
    "        \"time\": {\n",
    "            \"gte\": 0,\n",
    "            \"lt\": 10\n",
    "        }\n",
    "    }\n",
    "}\n",
    "data_fetcher = ElasticDataFetcher(es, \"lanl\", query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043bfb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "def pd_to_graph(auth_df, redteam_df):\n",
    "    srcs = []\n",
    "    dests = []\n",
    "    node_features = []\n",
    "    edge_attr = []\n",
    "    node_map = {}\n",
    "    y = []\n",
    "\n",
    "    # Merge the auth and redteam df\n",
    "    # time,user@domain,source computer,destination computer\n",
    "    auth_df = auth_df.merge(\n",
    "        redteam_df.assign(is_malicious=1),\n",
    "        on=['time', \"source computer\", 'destination computer'],\n",
    "        how='left'\n",
    "    )\n",
    "    auth_df['is_malicious'] = auth_df['is_malicious'].fillna(0).astype(int)\n",
    "\n",
    "    for _, row in auth_df.iterrows():\n",
    "        src, dest = row['source computer'], row['destination computer']\n",
    "        if src not in node_map:\n",
    "            node_map[src] = len(node_map)\n",
    "        if dest not in node_map:\n",
    "            node_map[dest] = len(node_map)\n",
    "        srcs.append(node_map[src])\n",
    "        dests.append(node_map[dest])\n",
    "        edge_attr.append([])\n",
    "        y.append(row[\"is_malicious\"])\n",
    "        \n",
    "    node_features = [k for k, v in sorted(node_map.items(), key=lambda item: item[1])]\n",
    "    return Data(\n",
    "        x=torch.zeros(3),\n",
    "        edge_index=torch.tensor([srcs, dests], dtype=torch.long),\n",
    "        edge_attr=torch.tensor(edge_attr),\n",
    "        y=y\n",
    "    )\n",
    "\n",
    "def pd_to_temporal(df_auth, df_redteam, start_time, end_time, step):\n",
    "    temporal = []\n",
    "    for time in range(start_time, end_time, step):\n",
    "        auth = df_auth[df_auth[\"time\"] == time]\n",
    "        redteam = df_redteam[df_redteam[\"time\"] == time]\n",
    "        temporal.append(pd_to_graph(auth, redteam))\n",
    "    return temporal\n",
    "\n",
    "\n",
    "es = Elasticsearch(\"https://localhost:9200\", \n",
    "                   api_key=\"MGZGTDhaVUJHWEpfZm5CYVB1bXo6dXBxVk5ucF9Rc3F6dWh5RjVRVDQzUQ==\", \n",
    "                   verify_certs=False, \n",
    "                   ssl_show_warn=False,)\n",
    "query = {\n",
    "    \"range\": {\n",
    "        \"time\": {\n",
    "            \"gte\": 1758220,\n",
    "            \"lt\": 1758230\n",
    "        }\n",
    "    }\n",
    "}\n",
    "data_fetcher = ElasticDataFetcher(es, \"lanl\", query)\n",
    "df = pd.DataFrame(data_fetcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad5ae73",
   "metadata": {},
   "outputs": [],
   "source": [
    "auth_df = df[df[\"file\"] == \"auth\"]\n",
    "redteam_df = df[df[\"file\"] == \"redteam\"]\n",
    "\n",
    "data = pd_to_temporal(auth_df, redteam_df, 1758220, 1758230, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1f9341",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import torch_geometric\n",
    "\n",
    "g = torch_geometric.utils.to_networkx(data[6], to_undirected=True)\n",
    "nx.draw(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca77a63a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df14ea2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2352d35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.loader import DataLoader\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "class GNNEncoder(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, hidden_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return x  # node embeddings\n",
    "    \n",
    "class RNNEncoder(nn.Module):\n",
    "    def __init__(self, hidden_channels, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.GRU(hidden_channels, hidden_channels, num_layers=num_layers,  batch_first=True)\n",
    "\n",
    "    def forward(self, x_seq):\n",
    "        # x_seq shape: [num_nodes, sequence_length, hidden_channels]\n",
    "        output, final_state = self.rnn(x_seq)\n",
    "        return output, final_state  # optional depending on decoder\n",
    "    \n",
    "class DotProductDecoder(nn.Module):\n",
    "    def forward(self, z_src, z_dst):\n",
    "        return (z_src * z_dst).sum(dim=1)\n",
    "    \n",
    "class FullModel(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.gnn = GNNEncoder(in_channels, hidden_channels)\n",
    "        self.rnn = RNNEncoder(hidden_channels)\n",
    "        self.decoder = DotProductDecoder()\n",
    "\n",
    "    def forward(self, graph_sequence, edge_pairs):\n",
    "        # Generate the features at each timestamp\n",
    "        # shape (timestamp, nodes, features)\n",
    "        graph_features = [self.gnn(data.x, data.edge_index) for data in graph_sequence]\n",
    "\n",
    "        # Switch the order, so that each node is treated as a batch\n",
    "        # shape (nodes, timestamp, features)\n",
    "        graph_features = torch.stack(graph_features)\n",
    "        graph_features = graph_features.permute(1, 0, 2)\n",
    "        rnn_output, _ = self.rnn(graph_features)\n",
    "\n",
    "        # Get the embeddings at time t+1\n",
    "        # shape (nodes, timestamp (t+1), features)\n",
    "        predicted_embeddings = rnn_output[:, -1, :]  # use last time step\n",
    "\n",
    "        # Generate the scores for \n",
    "        src = edge_pairs[0]\n",
    "        dst = edge_pairs[1]\n",
    "        scores = self.decoder(predicted_embeddings[src], predicted_embeddings[dst])  # shape [num_edges]\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bfd73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from utils.elastic_datafetcher import ElasticDataFetcher, LANL_Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "from torch_geometric.utils import to_undirected, negative_sampling\n",
    "\n",
    "model=FullModel(3, 15)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "es = Elasticsearch(\"https://localhost:9200\", \n",
    "                   api_key=\"MGZGTDhaVUJHWEpfZm5CYVB1bXo6dXBxVk5ucF9Rc3F6dWh5RjVRVDQzUQ==\", \n",
    "                   verify_certs=False, \n",
    "                   ssl_show_warn=False,)\n",
    "lanl_fetcher = LANL_Data(es, \"lanl-auth\")\n",
    "\n",
    "lanl_fetcher.get_nodemap()\n",
    "data = lanl_fetcher.fetch(0, 86400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966bf0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.snapshot(0, 604800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff1ea4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "batches = [([data.snapshot(i+t, i+t) for t in range (4)], data.snapshot(i+4, i+4)) for i in range(1758200, 1758300-5, 5)]\n",
    "for (graph_seq, y_graph) in batches:\n",
    "    positive_edges = y_graph.edge_index\n",
    "    negative_edges = negative_sampling(\n",
    "        edge_index=y_graph.edge_index,\n",
    "        num_nodes=y_graph.num_nodes,\n",
    "        num_neg_samples=positive_edges.size(1)\n",
    "    )\n",
    "\n",
    "    edge_pairs = torch.cat([positive_edges, negative_edges], dim=1)  # shape [2, num_edges]\n",
    "    labels = torch.cat([\n",
    "        torch.ones(positive_edges.size(1)),   # label 1 for real edges\n",
    "        torch.zeros(negative_edges.size(1))   # label 0 for fake edges\n",
    "    ])\n",
    "    scores = model(graph_seq, edge_pairs)\n",
    "    loss = loss_fn(scores, labels)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66613792",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aaaf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from elasticsearch import Elasticsearch\n",
    "# import time\n",
    "\n",
    "# old_index = \"lanl\"\n",
    "# new_index = \"lanl-auth\"\n",
    "\n",
    "# # Step 1: Define the correct mapping\n",
    "# new_mapping = {\n",
    "#     \"mappings\": {\n",
    "#         \"properties\": {\n",
    "#             \"time\": {\"type\": \"long\"},\n",
    "#             \"timestamp\": {\"type\": \"date\"},\n",
    "#             \"source computer\": {\"type\": \"keyword\"},\n",
    "#             \"destination computer\": {\"type\": \"keyword\"},\n",
    "#             \"source user@domain\": {\"type\": \"keyword\"},\n",
    "#             \"destination user@domain\": {\"type\": \"keyword\"},\n",
    "#             \"authentication type\": {\"type\": \"keyword\"},\n",
    "#             \"logon type\": {\"type\": \"keyword\"},\n",
    "#             \"authentication orientation\": {\"type\": \"keyword\"},\n",
    "#             \"success/failure\": {\"type\": \"keyword\"},\n",
    "#             \"file\": {\"type\": \"keyword\"}\n",
    "#         }\n",
    "#     }\n",
    "# }\n",
    "\n",
    "# # Step 2: Create the new index with the mapping\n",
    "# if not es.indices.exists(index=new_index):\n",
    "#     es.indices.create(index=new_index, body=new_mapping)\n",
    "#     print(f\"‚úÖ Created index: {new_index}\")\n",
    "# else:\n",
    "#     print(f\"‚ö†Ô∏è Index '{new_index}' already exists ‚Äî skipping creation\")\n",
    "\n",
    "# # Step 3: Reindex asynchronously to track progress\n",
    "# reindex_body = {\n",
    "#     \"source\": {\"index\": old_index},\n",
    "#     \"dest\": {\"index\": new_index}\n",
    "# }\n",
    "# response = es.reindex(body=reindex_body, wait_for_completion=False)\n",
    "# task_id = response[\"task\"]\n",
    "# print(f\"üöÄ Reindex started. Task ID: {task_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0242fca4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from utils.elastic_datafetcher import LANLGraphFetcher\n",
    "from elasticsearch import Elasticsearch\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.WARNING,  # Only show WARNING and above\n",
    "    format=\"%(levelname)s: %(message)s\"\n",
    ")\n",
    "logging.getLogger(\"elastic_transport.transport\").setLevel(logging.WARNING)\n",
    "\n",
    "es = Elasticsearch(\"http://localhost:9200\", \n",
    "                   api_key=\"MGZGTDhaVUJHWEpfZm5CYVB1bXo6dXBxVk5ucF9Rc3F6dWh5RjVRVDQzUQ==\", \n",
    "                   verify_certs=False, \n",
    "                   ssl_show_warn=False,)\n",
    "graph_data = LANLGraphFetcher(es, (\"lanl-auth\", \"lanl-redteam\") , 0, 1000, prefetch=True)\n",
    "\n",
    "all_data = []\n",
    "for data in tqdm(graph_data.grouped_by_second(), \"Loading graph data\", graph_data.to_sec-graph_data.from_sec):\n",
    "    all_data.append(data)\n",
    "all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f915468",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Parameters\n",
    "tsv_path = \"/data/LANL/auth.txt\"    # path to the input CSV file\n",
    "out_path = \"/data/LANL/auth.parquet\"  # path for the output Parquet file\n",
    "chunksize = 100000              # number of rows per batch\n",
    "\n",
    "# Ensure output directory exists\n",
    "o_dir = os.path.dirname(out_path)\n",
    "if o_dir and not os.path.exists(o_dir):\n",
    "    os.makedirs(o_dir)\n",
    "\n",
    "writer = None\n",
    "\n",
    "# Iterate over CSV in chunks\n",
    "enumerate_offset = 0\n",
    "for i, df_chunk in enumerate(pd.read_csv(tsv_path, chunksize=chunksize)):\n",
    "    # Convert pandas DataFrame chunk to Arrow Table\n",
    "    table = pa.Table.from_pandas(df_chunk)\n",
    "\n",
    "    # Initialize ParquetWriter with schema from first chunk\\    \n",
    "    if writer is None:\n",
    "        writer = pq.ParquetWriter(out_path, table.schema)\n",
    "\n",
    "    # Write batch to Parquet file\n",
    "    writer.write_table(table)\n",
    "    print(f\"Wrote chunk {i + 1} (rows {i * chunksize}‚Äì{(i + 1) * chunksize})\")\n",
    "\n",
    "# Close the writer when done\n",
    "if writer is not None:\n",
    "    writer.close()\n",
    "    print(f\"Parquet file saved to: {out_path}\")\n",
    "else:\n",
    "    print(\"No data was written; CSV may be empty.\")\n",
    "\n",
    "# Usage: \n",
    "# Once converted, load with pandas for fast querying:\n",
    "# df = pd.read_parquet(out_path)\n",
    "# e.g., df[df['column'] == 'value']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e6e108",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "out_path = \"/data/LANL/auth.parquet\"  # path for the output Parquet file\n",
    "\n",
    "df = pd.read_parquet(out_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f54b6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from elasticsearch import Elasticsearch, helpers\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Index name and CSV file path\n",
    "INDEX_NAME = \"lanl-redteam\"\n",
    "CSV_FILE_PATH = \"/data/LANL/redteam.txt\"\n",
    "REDTEAM_FILE_HEADERS = [\n",
    "    \"time\", \n",
    "    \"user@domain\", \n",
    "    \"source computer\", \n",
    "    \"destination computer\"\n",
    "]\n",
    "start_of_2015 = datetime(2015, 1, 1)\n",
    "\n",
    "# Step 1: Delete all documents in the index\n",
    "def delete_all_documents(index):\n",
    "    if es.indices.exists(index=index):\n",
    "        logger.info(f\"Deleting all documents from index: {index}\")\n",
    "        es.delete_by_query(\n",
    "            index=index,\n",
    "            body={\"query\": {\"match_all\": {}}},\n",
    "            refresh=True,\n",
    "            conflicts=\"proceed\"\n",
    "        )\n",
    "    else:\n",
    "        logger.warning(f\"Index {index} does not exist. Creating it.\")\n",
    "        es.indices.create(index=index)\n",
    "\n",
    "# Step 2: Load CSV\n",
    "def read_csv(path):\n",
    "    logger.info(f\"Reading CSV from: {path}\")\n",
    "    return pd.read_csv(path, names=REDTEAM_FILE_HEADERS)\n",
    "\n",
    "# Step 3: Push documents to Elasticsearch\n",
    "def index_csv(df, index):\n",
    "    logger.info(f\"Indexing {len(df)} documents into index: {index}\")\n",
    "\n",
    "    def generate_actions():\n",
    "        for row in df.to_dict(orient=\"records\"):\n",
    "            # Add custom fields\n",
    "            row[\"timestamp\"] = start_of_2015 + timedelta(seconds=row[\"time\"])\n",
    "            row[\"file\"] = \"auth\"\n",
    "\n",
    "            yield {\n",
    "                \"_index\": index,\n",
    "                \"_source\": row\n",
    "            }\n",
    "\n",
    "    helpers.bulk(es, generate_actions())\n",
    "    logger.info(\"Indexing complete.\")\n",
    "\n",
    "# Execute steps\n",
    "delete_all_documents(INDEX_NAME)\n",
    "df = read_csv(CSV_FILE_PATH)\n",
    "index_csv(df, INDEX_NAME)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fffe75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thristov/Try1/.venv/lib/python3.12/site-packages/torch_geometric/typing.py:68: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: /home/thristov/Try1/.venv/lib/python3.12/site-packages/libpyg.so: undefined symbol: _ZNK5torch8autograd4Node4nameEv\n",
      "  warnings.warn(f\"An issue occurred while importing 'pyg-lib'. \"\n",
      "/home/thristov/Try1/.venv/lib/python3.12/site-packages/torch_geometric/typing.py:86: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: /home/thristov/Try1/.venv/lib/python3.12/site-packages/torch_scatter/_version_cpu.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSsb\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-scatter'. \"\n",
      "/home/thristov/Try1/.venv/lib/python3.12/site-packages/torch_geometric/typing.py:97: UserWarning: An issue occurred while importing 'torch-cluster'. Disabling its usage. Stacktrace: /home/thristov/Try1/.venv/lib/python3.12/site-packages/torch_cluster/_version_cpu.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSsb\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-cluster'. \"\n",
      "/home/thristov/Try1/.venv/lib/python3.12/site-packages/torch_geometric/typing.py:113: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: /home/thristov/Try1/.venv/lib/python3.12/site-packages/torch_spline_conv/_version_cpu.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSsb\n",
      "  warnings.warn(\n",
      "/home/thristov/Try1/.venv/lib/python3.12/site-packages/torch_geometric/typing.py:124: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: /home/thristov/Try1/.venv/lib/python3.12/site-packages/torch_sparse/_version_cpu.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKSsb\n",
      "  warnings.warn(f\"An issue occurred while importing 'torch-sparse'. \"\n",
      "Loading graph data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 77.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "....."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading graph data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [00:00<00:00, 78.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".....Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from utils.elastic_datafetcher import LANLGraphFetcher\n",
    "from elasticsearch import Elasticsearch\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG, format=\"%(levelname)s: %(message)s\")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "es = Elasticsearch(\"http://localhost:9200\",\n",
    "                   api_key=\"MGZGTDhaVUJHWEpfZm5CYVB1bXo6dXBxVk5ucF9Rc3F6dWh5RjVRVDQzUQ==\",\n",
    "                   verify_certs=False,\n",
    "                   ssl_show_warn=False,)\n",
    "graph_data = LANLGraphFetcher(es, (\"lanl-auth\", \"lanl-redteam\") , 0, 5, prefetch=True)\n",
    "\n",
    "for data in tqdm(graph_data, \"Loading graph data\"):\n",
    "    print(\".\", end=\"\")\n",
    "\n",
    "for data in tqdm(graph_data, \"Loading graph data\"):\n",
    "    print(\".\", end=\"\")\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa7ded6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(b)[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e25cb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data before transformation has 3 edges and 3 time steps\n",
      "KeysView({'edge_index': tensor([[1, 2, 2],\n",
      "        [2, 2, 2]]), 'time': tensor([1., 1., 0.])})\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for dimension 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 69\u001b[39m\n\u001b[32m     67\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mData before transformation has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata.num_edges\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m edges and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data.time)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m time steps\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     68\u001b[39m removeLoops = RemoveDuplicatedEdgesTemporal(key=[\u001b[33m\"\u001b[39m\u001b[33medge_index\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m data = \u001b[43mremoveLoops\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mData after transformation has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata.num_edges\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m edges and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data.time)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m time steps\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch_geometric/transforms/base_transform.py:32\u001b[39m, in \u001b[36mBaseTransform.__call__\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: Any) -> Any:\n\u001b[32m     31\u001b[39m     \u001b[38;5;66;03m# Shallow-copy the data so that we prevent in-place data modification.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mRemoveDuplicatedEdgesTemporal.forward\u001b[39m\u001b[34m(self, data)\u001b[39m\n\u001b[32m     45\u001b[39m size = [s \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m store.size() \u001b[38;5;28;01mif\u001b[39;00m s \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m     46\u001b[39m num_nodes = \u001b[38;5;28mmax\u001b[39m(size) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) > \u001b[32m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m store.edge_index, edge_attrs = \u001b[43mcoalesce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m    \u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m.\u001b[49m\u001b[43medge_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m    \u001b[49m\u001b[43medge_attr\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstore\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, edge_attr \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(keys, edge_attrs):\n\u001b[32m     56\u001b[39m     store[key] = edge_attr\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/torch_geometric/utils/_coalesce.py:157\u001b[39m, in \u001b[36mcoalesce\u001b[39m\u001b[34m(edge_index, edge_attr, num_nodes, reduce, is_sorted, sort_by_row)\u001b[39m\n\u001b[32m    155\u001b[39m         edge_attr = edge_attr[perm]\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(edge_attr, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m         edge_attr = [\u001b[43me\u001b[49m\u001b[43m[\u001b[49m\u001b[43mperm\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m edge_attr]\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.jit.is_scripting() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(edge_index, EdgeIndex):\n\u001b[32m    160\u001b[39m     edge_index._sort_order = SortOrder(\u001b[33m'\u001b[39m\u001b[33mrow\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sort_by_row \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mcol\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mIndexError\u001b[39m: index 2 is out of bounds for dimension 0 with size 2"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.transforms import RemoveSelfLoops\n",
    "from typing import List, Union\n",
    "\n",
    "from torch_geometric.data import Data, HeteroData\n",
    "from torch_geometric.data.datapipes import functional_transform\n",
    "from torch_geometric.transforms import BaseTransform\n",
    "from torch_geometric.utils import coalesce\n",
    "\n",
    "\n",
    "class RemoveDuplicatedEdgesTemporal(BaseTransform):\n",
    "    r\"\"\"Removes duplicated edges from a given homogeneous or heterogeneous\n",
    "    graph. Useful to clean-up known repeated edges/self-loops in common\n",
    "    benchmark datasets, *e.g.*, in :obj:`ogbn-products`.\n",
    "    (functional name: :obj:`remove_duplicated_edges`).\n",
    "\n",
    "    Args:\n",
    "        key (str or [str], optional): The name of edge attribute(s) to merge in\n",
    "            case of duplication. (default: :obj:`[\"edge_weight\", \"edge_attr\"]`)\n",
    "        reduce (str, optional): The reduce operation to use for merging edge\n",
    "            attributes (:obj:`\"add\"`, :obj:`\"mean\"`, :obj:`\"min\"`,\n",
    "            :obj:`\"max\"`, :obj:`\"mul\"`). (default: :obj:`\"add\"`)\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        key: Union[str, List[str]] = ['edge_attr', 'edge_weight'],\n",
    "        reduce: str = \"add\",\n",
    "    ) -> None:\n",
    "        if isinstance(key, str):\n",
    "            key = [key]\n",
    "\n",
    "        self.keys = key\n",
    "        self.reduce = reduce\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        data: Union[Data, HeteroData],\n",
    "    ) -> Union[Data, HeteroData]:\n",
    "\n",
    "        for store in data.edge_stores:\n",
    "            keys = [key for key in self.keys if key in store]\n",
    "            print(store.keys())\n",
    "\n",
    "            size = [s for s in store.size() if s is not None]\n",
    "            num_nodes = max(size) if len(size) > 0 else None\n",
    "\n",
    "            store.edge_index, edge_attrs = coalesce(\n",
    "                edge_index=store.edge_index,\n",
    "                edge_attr=[store[key] for key in keys],\n",
    "                num_nodes=num_nodes,\n",
    "                reduce=self.reduce,\n",
    "            )\n",
    "\n",
    "            for key, edge_attr in zip(keys, edge_attrs):\n",
    "                store[key] = edge_attr\n",
    "\n",
    "        return data\n",
    "\n",
    "data = Data(\n",
    "    edge_index=torch.tensor([\n",
    "        [1,2,2],\n",
    "        [2,2,2],\n",
    "    ]),\n",
    "    time=torch.Tensor([1,1,0])\n",
    ")\n",
    "print(f\"Data before transformation has {data.num_edges} edges and {len(data.time)} time steps\")\n",
    "removeLoops = RemoveDuplicatedEdgesTemporal(key=[\"edge_index\"])\n",
    "data = removeLoops(data)\n",
    "print(f\"Data after transformation has {data.num_edges} edges and {len(data.time)} time steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30ba8f3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.num_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c724220",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
